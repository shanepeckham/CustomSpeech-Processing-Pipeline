import spacy
from fuzzywuzzy import fuzz
from fuzzywuzzy import process
import pandas as pd
from difflib import SequenceMatcher
import codecs
import jellyfish
from spacy.matcher import PhraseMatcher
import string
import numpy as np
import sys, getopt

nlp = spacy.load('en_core_web_lg')
nlp_api_response = nlp('')


def compute_ratio(text, api_text):
    return ({'Al': 'fuzzratio', 'Text': text, 'Score': fuzz.ratio(api_text, text)})


def compute_jaro_distance(text, api_text):
    return ({'Al': 'jellyfishjaro', 'Text': text, 'Score': (jellyfish.jaro_distance(api_text, text) * 100)})


def compute_partial_ratio(text, api_text):
    return ({'Al': 'fuzzpartialratio', 'Text': text, 'Score': fuzz.partial_ratio(api_text, text)})


def compute_semantic_similarity(segment, api_text):
    target_segment = nlp(segment)
    similarity = nlp_api_response.similarity(target_segment)
    return ({'Al': 'Spacy', 'Text': target_segment.text, 'Score': similarity * 100})


def compute_sequencematcher(text, api_text):
    ratio = SequenceMatcher(None, text, api_text).ratio()
    return ({'Al': 'Ratio2', 'Text': text, 'Score': ratio})


def append_fuzzy_extract(result, api_text):
    return ({'Al': 'Fuzzyprocess', 'Text': result[0], 'Score': (result[1])})


def search_target(target_sentence, api_text, nlp_api_response):
    match_results = []

    match_results.extend(list(map(compute_ratio, target_sentence, api_text)))
    match_results.extend(list(map(compute_partial_ratio, target_sentence, api_text)))
    fuzzy_results = process.extract(api_text, target_sentence)
    match_results.extend(list(map(append_fuzzy_extract, fuzzy_results, api_text)))
    match_results.extend(list(map(compute_semantic_similarity, target_sentence, api_text)))
    df_match_results = pd.DataFrame.from_dict(match_results)
    df_final_results = df_match_results.sort_values(by='Score', ascending=False)
    match_results = []

    for row in df_final_results.itertuples():
        match_results.append(compute_sequencematcher(row.Text, api_text))

    df_match_results = pd.DataFrame.from_dict(match_results)
    df_final_result = df_match_results.sort_values(by='Score', ascending=False)
    match = df_final_result.nlargest(1, 'Score')

    return match


def main(argv):

    offset_word = 0
    api_results = {}
    score_results = []
    algo_results = []
    previous_score = 0
    files_transcribed = 0
    files_action_needed = 0
    files_silent = 0
    global nlp_api_response

    # Slide sentences to the left and right for better matching but uses more CPU
    WORD_SLIDE = 20

    # Pad number of words onto left and right for better matching but uses more CPU
    WORD_PAD = 20

    # When we are doing substring searches as a last resort - do not change 3 is ideal
    SUBSTRING_PAD = 3

    # This is your transcribed file for the video. Format is text only. No need for
    # speaker names or timings, only the continuous transcription text.
    TRANSCRIBED_FILE = '/Users/shanepeckham/sources/video/File/11_WTA_ROM_STEPvGARC_2018/11_WTA_ROM_STEPvGARC_2018.txt'

    # This is the file that is automatically generated by the Speech to Text process
    AUDIO_PROCESSED_FILE = '/Users/shanepeckham/sources/video/File/11_WTA_ROM_STEPvGARC_2018/11_WTA_ROM_STEPvGARC_OFFSET.txt'

    # This is the automated transcript that will be generated
    GENERATED_TRANSCRIPT_FILE = '/Users/shanepeckham/sources/video/Results2/11_WTA_ROM_STEPvGARC_TRANSCRIPT_OUTPUT.txt'

    # This is the second pass threshold using broader semantic search and fuzzy matching - the higher the value the more precise but
    # the more restrictive the match criteria.The recommended value is 80%. Format - enter 80 for 80%
    PASS2_THRESHOLD = 80

    # This is the third pass threshold using expression and pattern matching - the higher the value the more precise but
    # the more restrictive the match criteria. The recommended value is 72%. Format - enter 72 for 72%
    PASS3_THRESHOLD = 72

    try:
        print("Getting args")
        opts, args = getopt.getopt(argv, "t:a:g:", ["TRANSCRIBED=", "AUDIO_PROCESSED=", "GENERATED_TRANSCRIPT="])
        print(opts, args)
    except getopt.GetoptError:
        print("Arguments required for transcribed file -t, audio processed file -a and generated transcript file -g")
        sys.exit(2)
    for opt, arg in opts:
        if opt == '-t':
            print("-t", arg)
            TRANSCRIBED_FILE = arg
        elif opt == "-a":
            print("-a", arg)
            AUDIO_PROCESSED_FILE = arg
        elif opt == "-g":
            print("-g", arg)
            GENERATED_TRANSCRIPT_FILE = arg

    print("Open", TRANSCRIBED_FILE)
    with open(TRANSCRIBED_FILE, "r", encoding='utf-8-sig') as transcribed_file:
        transcribed_text = transcribed_file.read().replace('\n', ' ')

    target = nlp(transcribed_text)
    target_length = len(target)
    word_count = 0

    table = str.maketrans(dict.fromkeys(string.punctuation))

    with codecs.open(GENERATED_TRANSCRIPT_FILE, "w", "utf-8-sig") as transcript:
        with codecs.open(AUDIO_PROCESSED_FILE, "r", "utf-8") as audio_processed_file:

            for line in audio_processed_file:
                line_read = line.split("\t")
                ticks_offset = line_read[0]
                filename = line_read[1]

                api_text_d = line_read[2][:]
                api_text = api_text_d.replace('\n', '')
                api_text = api_text.replace('\r', '')
                nlp_api_response = nlp(api_text)
                nlp_api_response_len = len(nlp_api_response)
                print("Processing file: ", filename)
                print("***API Response*** for ", api_text)

                source = nlp(api_text)
                source_length = len(source)

                # Let's segment into sentences
                target_sentence = []

                if offset_word == 0:
                    target_sentence.append(target[:nlp_api_response_len].text)
                else:
                    target_sentence.append(target[offset_word:(nlp_api_response_len + offset_word) + 1].text)
                    if offset_word - 1 > 0:
                        target_sentence.append(target[(offset_word - 1):(nlp_api_response_len + offset_word)].text)

                match = search_target(target_sentence, api_text, nlp_api_response)

                # Pass 1 - Semantic and fuzzy search
                if match['Score'][match.index.values[0]] < (PASS2_THRESHOLD / 100):
                    previous_score = match['Score'][match.index.values[0]]
                    print("No match: Pass 1 Semantic and fuzzy search")
                    i = 0
                    target_sentence = [target[:nlp_api_response_len].text]
                    while i < WORD_SLIDE:  # Let's slide 20 words either way
                        for j in range(WORD_PAD):  # Let's pad extra words on either side
                            target_sentence.append(
                                target[(offset_word - i):(nlp_api_response_len + offset_word + j)].text)
                            target_sentence.append(
                                target[(offset_word + i):(nlp_api_response_len + offset_word + j)].text)
                        i += 1

                    # Pass 2 - Wider semantic and fuzzy search
                    match = search_target(target_sentence, api_text, nlp_api_response)
                    print("Pass 1 Accuracy:", '{:0.0f}%'.format(previous_score * 100), "Pass 2 Accuracy:",
                          '{:0.0f}%'.format(match['Score'][match.index.values[0]] * 100))
                    previous_score = match['Score'][match.index.values[0]]

                    # Pass 3 Let's get medieval - E.g we need to match strings and patterns
                    if match['Score'][match.index.values[0]] < (PASS3_THRESHOLD / 100):
                        print("No match: Pass 2 Wider Semantic and fuzzy search")
                        target_sentence = []
                        response_list = []
                        j = 1

                        if nlp_api_response_len > 2:
                            matcher = PhraseMatcher(nlp.vocab)
                            cleaned = api_text.translate(table)

                            nlp_api_response_clean = nlp(cleaned)
                            nlp_api_response_clean_len = len(nlp_api_response_clean)
                            j = 0
                            matcher = PhraseMatcher(nlp.vocab)

                            # This may be ugly but we need the text as is as we need to do some basic
                            # string matching and pattern expressions here
                            while j < (nlp_api_response_clean_len / 3):

                                if nlp_api_response_clean[j:SUBSTRING_PAD + j].text != "":
                                    response_list.append(nlp_api_response_clean[j:SUBSTRING_PAD + j].text)
                                if nlp_api_response_clean[j - 1:SUBSTRING_PAD + j - 1].text != "":
                                    response_list.append(nlp_api_response_clean[j - 1:SUBSTRING_PAD + j - 1].text)
                                if nlp_api_response_clean[-SUBSTRING_PAD - j:-j].text != "":
                                    response_list.append(nlp_api_response_clean[-SUBSTRING_PAD - j:-j].text)
                                j += 1

                            patterns = [nlp(text) for text in response_list]

                            if len(response_list) > 2:
                                matcher.add('ResponseList', None, *patterns)
                            else:
                                matcher.add('API', None, nlp_api_response_clean)

                            # Search the remaining text
                            doc = nlp(target[offset_word:].text)
                            matches = matcher(doc)

                            if not matches:
                                print("No matching text NLP Matcher - Needs manual check")  # CHANGE
                                transcript.write(filename + "\t" + "NEEDS MANUAL CHECK" + "\n")
                                files_action_needed += 1
                                continue

                            for match_id, start, end in matches:
                                span = doc[start:end]
                                for i in range(SUBSTRING_PAD):
                                    target_sentence.append(target[
                                                            offset_word + start:offset_word + start + nlp_api_response_len - SUBSTRING_PAD].text)
                                    target_sentence.append(target[
                                                            offset_word + start:offset_word + start + nlp_api_response_len + SUBSTRING_PAD].text)

                            target_sentence = list(set(target_sentence))
                            match = search_target(target_sentence, api_text, nlp_api_response)

                            print("Pass 2 Accuracy:", '{:0.0f}%'.format(previous_score * 100), "Pass 3 Accuracy:",
                                  '{:0.0f}%'.format(match['Score'][match.index.values[0]] * 100))

                            if match['Score'][match.index.values[0]] < 0.5:
                                # Give up
                                print("No matching text Recurse - Needs manual check")  # CHANGE
                                transcript.write(filename + "\t" + "NEEDS MANUAL CHECK" + "\n")
                                files_action_needed += 1
                                continue

                print("***Matched text***", match['Text'][match.index.values[0]])
                print("Transcription Accuracy:", '{:0.0f}%'.format(match['Score'][match.index.values[0]] * 100))
                print("\n")
                score_results.append(match['Score'][match.index.values[0]])
                files_transcribed += 2
                algo_results.append(match['Al'][match.index.values[0]])

            #    if int(ticks_offset) < 1000:
            #        transcript.write(
            #            filename + "\t" + "<OVERLAP> " + match['Text'][match.index.values[0]] + " <OVERLAP/>" + "\n")
            #        offset_word -= 2
            #        api_results.update({filename: "<OVERLAP> " + api_text + " <OVERLAP/>"})
            #    else:
                transcript.write(filename + "\t" + match['Text'][match.index.values[0]] + "\n")
                api_results.update({filename: api_text})

                nlp_offset_word = nlp(match['Text'][match.index.values[0]])
                nlp_offset_word_len = len(nlp_offset_word) - 2
                offset_word = nlp_offset_word_len + offset_word

    print("Transcript file generated:" + GENERATED_TRANSCRIPT_FILE)
    print("\nProcessed", str(len(score_results)), "files with translation accuracy of",
          str(np.mean(score_results) * 100) + "%")


if __name__ == "__main__":
    main(sys.argv[1:])
